{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ONCBdR2VvZI",
    "outputId": "e935bfa6-a202-49c6-f11e-e8a5a3fe3875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark==3.4.1 in c:\\users\\depen\\appdata\\roaming\\python\\python312\\site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\depen\\appdata\\roaming\\python\\python312\\site-packages (from pyspark==3.4.1) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class DataPreprocessing\n",
    "\n",
    "class DataPreprocessing:\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Function to Read CSV file\n",
    "\n",
    "    def readCsv(self, path, spark=None):\n",
    "        from pyspark.sql import SparkSession  # Module to Create Spark Session\n",
    "\n",
    "        if spark is None:\n",
    "            spark = SparkSession.builder.appName(\n",
    "                \"DataPreprocessing\").getOrCreate()\n",
    "\n",
    "        # Read the CSV file with header and infer schema\n",
    "        df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "        for col in df.columns:  # Rename columns with dots\n",
    "            df = df.withColumnRenamed(col, col.replace('.', '_'))\n",
    "        return df\n",
    "\n",
    "# Function to Fill Missing Values\n",
    "\n",
    "    def fillMissingValues(self, df, numeric_cols, categorical_cols):\n",
    "\n",
    "      import pyspark.sql.functions as F # Module to perform various operations on DataFrame\n",
    "\n",
    "      df = df.replace(['?', 'NA', 'Nan', 'na', 'NaN'], None)  # Replace missing values with None\n",
    "\n",
    "      for col in categorical_cols:  # Fill with the most frequent value\n",
    "        mode_value = df.select(F.mode(df[col])).first()[0]  # Calculate the mode value\n",
    "        df = df.fillna(mode_value, subset=[col])  # Fill with the mode value\n",
    "\n",
    "      for col in numeric_cols:  # Fill with the mean value\n",
    "        mean_value = df.select(F.mean(df[col])).first()[0]  # Calculate the mean value\n",
    "        df = df.fillna(mean_value, subset=[col])  # Fill with the mean value\n",
    "\n",
    "      return df\n",
    "\n",
    "# Function to Remove Outliers using IQR(Interquartile Range)\n",
    "\n",
    "    def removeOutliers(self, df, cols):\n",
    "        \n",
    "        from pyspark.sql.functions import col # Module to perform various operations on DataFrame\n",
    "\n",
    "        # Calculate the IQR for each column\n",
    "        for col_name in cols:\n",
    "          q1 = df.approxQuantile(col_name, [0.25], 0.05)[0]\n",
    "          q3 = df.approxQuantile(col_name, [0.75], 0.05)[0]\n",
    "          iqr = q3 - q1\n",
    "          # Remove outliers using IQR\n",
    "          lower_bound = q1 - 1.5 * iqr\n",
    "          upper_bound = q3 + 1.5 * iqr\n",
    "          # Filter out the outliers\n",
    "          df = df.filter((col(col_name) >= lower_bound) & (col(col_name) <= upper_bound))\n",
    "        return df\n",
    "\n",
    "# Function to Remove Outliers using Z-Score Method\n",
    "\n",
    "    def RemoveOutliers_ZScore(self, df, columns):\n",
    "\n",
    "        from pyspark.sql.functions import col # Module to perform various operations on DataFrame\n",
    "\n",
    "        for col_name in columns:\n",
    "            # Calculate mean and standard deviation for the column\n",
    "            mean_value = df.selectExpr(f'mean({col_name}) as mean').collect()[0]['mean']\n",
    "            stddev_value = df.selectExpr(f'stddev({col_name}) as stddev').collect()[0]['stddev']\n",
    "\n",
    "            # Remove outliers using Z-score\n",
    "            threshold = 3 * stddev_value\n",
    "            # Filter out the outliers\n",
    "            df = df.filter(col(col_name).between(mean_value - threshold, mean_value + threshold))\n",
    "\n",
    "        return df\n",
    "\n",
    "# Function to Normalize Numeric Columns using MinMaxScalar\n",
    "\n",
    "    def NormalizeNumericColumns(self, df, cols):\n",
    "\n",
    "      from pyspark.ml import Pipeline # Module to create a pipeline of stages\n",
    "\n",
    "      from pyspark.ml.feature import VectorAssembler, MinMaxScaler # Module to perform various feature engineering operations\n",
    "\n",
    "      # VectorAssembler to Assemble Columns in a Single Column named Features\n",
    "      assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "      # Apply MinMaxScaler to the vector column\n",
    "      scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"Scaledfeatures\")\n",
    "\n",
    "      # Create a pipeline\n",
    "      pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "      # Fit the pipeline and transform the data\n",
    "      pipeline_model = pipeline.fit(df)\n",
    "      df = pipeline_model.transform(df)\n",
    "\n",
    "      df = df.drop(\"features\")\n",
    "\n",
    "      return df\n",
    "\n",
    "# Function to Standardize Numeric Columns using Standard Scaler\n",
    "\n",
    "    def StandardizeNumericColumns(self, df, cols):\n",
    "\n",
    "      from pyspark.ml import Pipeline # Module to create a pipeline of stages\n",
    "\n",
    "      from pyspark.ml.feature import VectorAssembler, StandardScaler # Module to perform various feature engineering operations\n",
    "\n",
    "      # VectorAssembler to Assemble Columns in a Single Column named Features\n",
    "      assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "      # Apply StandardScaler to the vector column\n",
    "      scaler = StandardScaler(inputCol=\"features\", outputCol=\"Scaledfeatures\", withMean=True, withStd=True)\n",
    "\n",
    "      # Create a pipeline\n",
    "      pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "      # Fit the pipeline and transform the data\n",
    "      pipeline_model = pipeline.fit(df)\n",
    "      df = pipeline_model.transform(df)\n",
    "\n",
    "      df = df.drop(\"features\")\n",
    "\n",
    "      return df\n",
    "\n",
    "# Function to Encode Categorical Columns using Label Encoding\n",
    "\n",
    "    def LabelEncoding(self, df, categorical_cols):\n",
    "\n",
    "      from pyspark.ml import Pipeline # Module to create a pipeline of stages\n",
    "\n",
    "      from pyspark.ml.feature import StringIndexer # Module to perform various feature engineering operations\n",
    "\n",
    "      from pyspark.sql.types import BooleanType # Module to specify the data type of a column\n",
    "\n",
    "      for column in categorical_cols:\n",
    "        # Check if the column has only two unique values\n",
    "        if df.schema[column].dataType == BooleanType():\n",
    "          df = df.withColumn(column, df[column].cast('int'))\n",
    "        else:\n",
    "          # StringIndexer to convert categorical column to numerical indices\n",
    "          indexer = StringIndexer(inputCol=column, outputCol=f\"{column}Index\")\n",
    "\n",
    "          # Create a pipeline\n",
    "          pipeline = Pipeline(stages=[indexer])\n",
    "\n",
    "          # Fit the pipeline and transform the data\n",
    "          pipeline_model = pipeline.fit(df)\n",
    "          df = pipeline_model.transform(df)\n",
    "\n",
    "          df = df.withColumn(column, df[f\"{column}Index\"]).drop(df[f\"{column}Index\"])\n",
    "\n",
    "          # indexer_model = pipeline_model.stages[0]  # Get the StringIndexerModel from the pipeline\n",
    "          # labels = indexer_model.labels  # Get the list of labels\n",
    "          # print(\"Category to Index Mapping:\")\n",
    "          # for index, label in enumerate(labels):\n",
    "          #   print(f\"{label}: {index}\")\n",
    "\n",
    "      return df\n",
    "\n",
    "# Function to Encode Categorical Columns using One Hot Encoding\n",
    "\n",
    "    def oneHotEncoding(self, df, categorical_cols):\n",
    "\n",
    "      from pyspark.ml import Pipeline # Module to create a pipeline of stages\n",
    "\n",
    "      from pyspark.ml.feature import StringIndexer, OneHotEncoder # Module to perform various feature engineering operations\n",
    "\n",
    "      from pyspark.sql.types import BooleanType # Module to specify the data type of a column\n",
    "\n",
    "      for column in categorical_cols:\n",
    "        # Check if the column has only two unique values\n",
    "        if df.schema[column].dataType == BooleanType():\n",
    "          df = df.withColumn(column, df[column].cast('int'))\n",
    "        else:\n",
    "          # String Indexing\n",
    "          indexer = StringIndexer(inputCol=column, outputCol=f\"{column}Index\")\n",
    "\n",
    "          # Hot Encoding\n",
    "          encoder = OneHotEncoder(inputCol=f\"{column}Index\", outputCol=f\"{column}Vec\")\n",
    "\n",
    "          # Create a pipeline\n",
    "          pipeline = Pipeline(stages=[indexer, encoder])\n",
    "\n",
    "          # Fit the pipeline and transform the data\n",
    "          pipeline_model = pipeline.fit(df)\n",
    "          df = pipeline_model.transform(df)\n",
    "\n",
    "          df = df.withColumn(column, df[f\"{column}Vec\"]).drop(df[f\"{column}Index\"], df[f\"{column}Vec\"])\n",
    "\n",
    "          # indexer_model = pipeline_model.stages[0]  # Get the StringIndexerModel from the pipeline\n",
    "          # labels = indexer_model.labels  # Get the list of labels\n",
    "          # print(\"Category to Index Mapping:\")\n",
    "          # for index, label in enumerate(labels):\n",
    "          #   print(f\"{label}: {index}\")\n",
    "\n",
    "      return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I4d8z8C-apxp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\depen\\AppData\\Local\\Temp\\ipykernel_16372\\3055393733.py:11: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  df = dp.readCsv(\"C:\\\\Users\\\\depen\\\\OneDrive\\\\Documents\\Datasets\\\\iris.csv\", spark)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  from pyspark.sql import SparkSession  # Module to Create Spark Session\n",
    "  import matplotlib.pyplot as plt  # Module to plot graphs\n",
    "\n",
    "\n",
    "  spark = SparkSession.builder.master(\"local\").appName(\"DataPreprocessing\").getOrCreate()\n",
    "  \n",
    "  dp = DataPreprocessing()\n",
    "\n",
    "  df = dp.readCsv(\"C:\\\\Users\\\\depen\\\\OneDrive\\\\Documents\\Datasets\\\\iris.csv\", spark)\n",
    "\n",
    "  target = df.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZCq27jV1M5cc"
   },
   "outputs": [],
   "source": [
    "  if 'id' in df.columns:\n",
    "    df = df.drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IR-k4Bbjh30p",
    "outputId": "f3d78f5a-ea6c-486a-eac3-4cd4091aceee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- variety: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oxoGH-P3yW72",
    "outputId": "b66f2215-da93-451b-bd50-cc8bd70841a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|   variety|count|\n",
      "+----------+-----+\n",
      "| Virginica|   50|\n",
      "|    Setosa|   50|\n",
      "|Versicolor|   50|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  distinct_values = df.groupBy(target).count()\n",
    "  distinct_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7m-Chc-ngWr",
    "outputId": "fa52f3f8-0004-4666-fe89-61fbba2d0297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "['variety']\n"
     ]
    }
   ],
   "source": [
    "  numeric_cols = []\n",
    "  categorical_cols = []\n",
    "\n",
    "  for column in df.dtypes:  # Check the data type of each column\n",
    "    if column[1] == 'string' or column[1] == 'boolean':  # If the data type is string, it is categorical\n",
    "      categorical_cols.append(column[0])\n",
    "    else:  # If the data type is numeric, it is numeric\n",
    "      numeric_cols.append(column[0])\n",
    "  print(numeric_cols)\n",
    "  print(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vA3UFp7uvbgC",
    "outputId": "6ce14e1f-092d-4742-8693-47ab67187f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "  print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BklDUWqmvNq2"
   },
   "outputs": [],
   "source": [
    "  df = dp.fillMissingValues(df, numeric_cols, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "k-nlua8WvevD",
    "outputId": "e1365262-99a3-451c-c2d5-0bb09e91317c"
   },
   "outputs": [],
   "source": [
    "  for column in numeric_cols:\n",
    "    mean_value_before = df.selectExpr(f'mean({column}) as mean').collect()[0]['mean']\n",
    "    stddev_value_before = df.selectExpr(f'stddev({column}) as stddev').collect()[0]['stddev']\n",
    "    # Plotting before outlier removal\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df.select(column).rdd.flatMap(lambda x: x).collect(), bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(x=mean_value_before, color='red', linestyle='--', label='Mean')\n",
    "    plt.axvline(x=mean_value_before + 3 * stddev_value_before, color='orange', linestyle='--', label='3 Std Dev')\n",
    "    plt.axvline(x=mean_value_before - 3 * stddev_value_before, color='orange', linestyle='--')\n",
    "    plt.title('Distribution Before Outlier Removal')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "AMhrLpxketY6"
   },
   "outputs": [],
   "source": [
    "  columns = numeric_cols[:]\n",
    "  if target in columns:\n",
    "    columns.remove(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBThIhaFUCUR",
    "outputId": "65baa98d-c409-4a73-d8ef-a2f1dc3712a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29829\n"
     ]
    }
   ],
   "source": [
    "  df = dp.RemoveOutliers_ZScore(df, columns)\n",
    "  print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "p9wnMoa6UFzC",
    "outputId": "4086156b-ec3a-40e5-82d3-f1b96ab2f21a"
   },
   "outputs": [],
   "source": [
    "  for column in numeric_cols:\n",
    "    mean_value_after = df.selectExpr(f'mean({column}) as mean').collect()[0]['mean']\n",
    "    stddev_value_after = df.selectExpr(f'stddev({column}) as stddev').collect()[0]['stddev']\n",
    "    # Plotting before outlier removal\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df.select(column).rdd.flatMap(lambda x: x).collect(), bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(x=mean_value_after, color='red', linestyle='--', label='Mean')\n",
    "    plt.axvline(x=mean_value_after + 3 * stddev_value_after, color='orange', linestyle='--', label='3 Std Dev')\n",
    "    plt.axvline(x=mean_value_after - 3 * stddev_value_after, color='orange', linestyle='--')\n",
    "    plt.title('Distribution after Outlier Removal')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZ9W6zNhYkeN",
    "outputId": "8e84d2be-7ff2-46dd-ecbe-6d283e44d23d"
   },
   "outputs": [],
   "source": [
    "  df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fa6nsfcHTBu-",
    "outputId": "098d5189-170f-4fe2-eb95-9fb37cb4a429"
   },
   "outputs": [],
   "source": [
    "  df = dp.oneHotEncoding(df, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnWEBJHFUTr3",
    "outputId": "a0f290e5-50fa-4e1d-a2fc-b8564885c0e9"
   },
   "outputs": [],
   "source": [
    "  df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "2WE8PWQUcQxQ"
   },
   "outputs": [],
   "source": [
    "  df = dp.LabelEncoding(df, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0E1NY3kcd-f",
    "outputId": "4b3888e7-8351-4b39-f657-7fa2c27a44b1"
   },
   "outputs": [],
   "source": [
    "  df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "2GF7Ct3LT6IK"
   },
   "outputs": [],
   "source": [
    "  columns.extend(categorical_cols)\n",
    "  if target in columns:\n",
    "    columns.remove(target)\n",
    "  df = dp.NormalizeNumericColumns(df, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPaEc3ccpADd",
    "outputId": "5179576a-eb90-4afc-cadf-861e9f1312a8"
   },
   "outputs": [],
   "source": [
    "  df.select(\"Scaledfeatures\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EFI1yQkKVTA-",
    "outputId": "7b6be903-e442-4e31-fcc5-b87c4c24d8e7"
   },
   "outputs": [],
   "source": [
    "  for column in numeric_cols:\n",
    "    mean_value_after = df.selectExpr(f'mean({column}) as mean').collect()[0]['mean']\n",
    "    stddev_value_after = df.selectExpr(f'stddev({column}) as stddev').collect()[0]['stddev']\n",
    "    # Plotting before outlier removal\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df.select(column).rdd.flatMap(lambda x: x).collect(), bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(x=mean_value_after, color='red', linestyle='--', label='Mean')\n",
    "    plt.axvline(x=mean_value_after + 3 * stddev_value_after, color='orange', linestyle='--', label='3 Std Dev')\n",
    "    plt.axvline(x=mean_value_after - 3 * stddev_value_after, color='orange', linestyle='--')\n",
    "    plt.title('Distribution after Outlier Removal')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bw9WDmBtwjsd"
   },
   "outputs": [],
   "source": [
    "  spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
