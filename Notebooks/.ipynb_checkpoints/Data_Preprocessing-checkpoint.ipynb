{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ONCBdR2VvZI",
        "outputId": "e935bfa6-a202-49c6-f11e-e8a5a3fe3875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pyspark in c:\\users\\depen\\appdata\\roaming\\python\\python312\\site-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\depen\\appdata\\roaming\\python\\python312\\site-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Class DataPreprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataPreprocessing:\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self) -> None:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5HXFWzEXfh9"
      },
      "source": [
        "# Function to Read CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iBwYxB11aWNm"
      },
      "outputs": [],
      "source": [
        "    def readCsv(self, path, spark=None):\n",
        "        from pyspark.sql import SparkSession  # Module to Create Spark Session\n",
        "\n",
        "        if spark is None:\n",
        "            spark = SparkSession.builder.appName(\n",
        "                \"DataPreprocessing\").getOrCreate()\n",
        "\n",
        "        # Read the CSV file with header and infer schema\n",
        "        df = spark.read.csv(path, header=True, inferSchema=True)\n",
        "\n",
        "        for col in df.columns:  # Rename columns with dots\n",
        "            df = df.withColumnRenamed(col, col.replace('.', '_'))\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NRQwJKyXsFr"
      },
      "source": [
        "# Function to Fill Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zSzJv6qnst2x"
      },
      "outputs": [],
      "source": [
        "    def fillMissingValues(self, df, numeric_cols, categorical_cols):\n",
        "\n",
        "      import pyspark.sql.functions as F # Module to perform various operations on DataFrame\n",
        "\n",
        "      df = df.replace(['?', 'NA', 'Nan', 'na', 'NaN'], None)  # Replace missing values with None\n",
        "\n",
        "      for col in categorical_cols:  # Fill with the most frequent value\n",
        "        mode_value = df.select(F.mode(df[col])).first()[0]  # Calculate the mode value\n",
        "        df = df.fillna(mode_value, subset=[col])  # Fill with the mode value\n",
        "\n",
        "      for col in numeric_cols:  # Fill with the mean value\n",
        "        mean_value = df.select(F.mean(df[col])).first()[0]  # Calculate the mean value\n",
        "        df = df.fillna(mean_value, subset=[col])  # Fill with the mean value\n",
        "\n",
        "      return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDq31RKCZcO1"
      },
      "source": [
        "# Function to Remove Outliers using IQR(Interquartile Range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YO_8cl0LZZE9"
      },
      "outputs": [],
      "source": [
        "    def removeOutliers(self, df, cols):\n",
        "        \n",
        "        from pyspark.sql.functions import col # Module to perform various operations on DataFrame\n",
        "\n",
        "        # Calculate the IQR for each column\n",
        "        for col_name in cols:\n",
        "          q1 = df.approxQuantile(col_name, [0.25], 0.05)[0]\n",
        "          q3 = df.approxQuantile(col_name, [0.75], 0.05)[0]\n",
        "          iqr = q3 - q1\n",
        "          # Remove outliers using IQR\n",
        "          lower_bound = q1 - 1.5 * iqr\n",
        "          upper_bound = q3 + 1.5 * iqr\n",
        "          # Filter out the outliers\n",
        "          df = df.filter((col(col_name) >= lower_bound) & (col(col_name) <= upper_bound))\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klu1-HinuF3e"
      },
      "source": [
        "# Function to Remove Outliers using Z-Score Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zbzUSItvkp91"
      },
      "outputs": [],
      "source": [
        "    def RemoveOutliers_ZScore(self, df, columns):\n",
        "\n",
        "        from pyspark.sql.functions import col # Module to perform various operations on DataFrame\n",
        "\n",
        "        for col_name in columns:\n",
        "            # Calculate mean and standard deviation for the column\n",
        "            mean_value = df.selectExpr(f'mean({col_name}) as mean').collect()[0]['mean']\n",
        "            stddev_value = df.selectExpr(f'stddev({col_name}) as stddev').collect()[0]['stddev']\n",
        "\n",
        "            # Remove outliers using Z-score\n",
        "            threshold = 3 * stddev_value\n",
        "            # Filter out the outliers\n",
        "            df = df.filter(col(col_name).between(mean_value - threshold, mean_value + threshold))\n",
        "\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8xt5WnCTvi1"
      },
      "source": [
        "# Function to Normalize Numeric Columns using MinMaxScalar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HAAWwyv-N9NS"
      },
      "outputs": [],
      "source": [
        "    def NormalizeNumericColumns(self, df, cols):\n",
        "\n",
        "      from pyspark.ml import Pipeline # Module to create a pipeline of stages\n",
        "\n",
        "      from pyspark.ml.feature import VectorAssembler, MinMaxScaler # Module to perform various feature engineering operations\n",
        "\n",
        "      # VectorAssembler to Assemble Columns in a Single Column named Features\n",
        "      assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
        "\n",
        "      # Apply MinMaxScaler to the vector column\n",
        "      scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"Scaledfeatures\")\n",
        "\n",
        "      # Create a pipeline\n",
        "      pipeline = Pipeline(stages=[assembler, scaler])\n",
        "\n",
        "      # Fit the pipeline and transform the data\n",
        "      pipeline_model = pipeline.fit(df)\n",
        "      df = pipeline_model.transform(df)\n",
        "\n",
        "      df = df.drop(\"features\")\n",
        "\n",
        "      return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wahW-nezZUdv"
      },
      "source": [
        "# Function to Standardize Numeric Columns using Standard Scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fQe1-im1XAb7"
      },
      "outputs": [],
      "source": [
        "    def StandardizeNumericColumns(self, df, cols):\n",
        "\n",
        "      from pyspark.ml import Pipeline # Module to create a pipeline of stages\n",
        "\n",
        "      from pyspark.ml.feature import VectorAssembler, StandardScaler # Module to perform various feature engineering operations\n",
        "\n",
        "      # VectorAssembler to Assemble Columns in a Single Column named Features\n",
        "      assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
        "\n",
        "      # Apply StandardScaler to the vector column\n",
        "      scaler = StandardScaler(inputCol=\"features\", outputCol=\"Scaledfeatures\", withMean=True, withStd=True)\n",
        "\n",
        "      # Create a pipeline\n",
        "      pipeline = Pipeline(stages=[assembler, scaler])\n",
        "\n",
        "      # Fit the pipeline and transform the data\n",
        "      pipeline_model = pipeline.fit(df)\n",
        "      df = pipeline_model.transform(df)\n",
        "\n",
        "      df = df.drop(\"features\")\n",
        "\n",
        "      return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ranJyV8cFFe"
      },
      "source": [
        "# Function to Encode Categorical Columns using Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QEhpAFuzJAV-"
      },
      "outputs": [],
      "source": [
        "    def LabelEncoding(self, df, categorical_cols):\n",
        "\n",
        "      from pyspark.ml import Pipeline # Module to create a pipeline of stages\n",
        "\n",
        "      from pyspark.ml.feature import StringIndexer # Module to perform various feature engineering operations\n",
        "\n",
        "      from pyspark.sql.types import BooleanType # Module to specify the data type of a column\n",
        "\n",
        "      for column in categorical_cols:\n",
        "        # Check if the column has only two unique values\n",
        "        if df.schema[column].dataType == BooleanType():\n",
        "          df = df.withColumn(column, df[column].cast('int'))\n",
        "        else:\n",
        "          # StringIndexer to convert categorical column to numerical indices\n",
        "          indexer = StringIndexer(inputCol=column, outputCol=f\"{column}Index\")\n",
        "\n",
        "          # Create a pipeline\n",
        "          pipeline = Pipeline(stages=[indexer])\n",
        "\n",
        "          # Fit the pipeline and transform the data\n",
        "          pipeline_model = pipeline.fit(df)\n",
        "          df = pipeline_model.transform(df)\n",
        "\n",
        "          df = df.withColumn(column, df[f\"{column}Index\"]).drop(df[f\"{column}Index\"])\n",
        "\n",
        "          # indexer_model = pipeline_model.stages[0]  # Get the StringIndexerModel from the pipeline\n",
        "          # labels = indexer_model.labels  # Get the list of labels\n",
        "          # print(\"Category to Index Mapping:\")\n",
        "          # for index, label in enumerate(labels):\n",
        "          #   print(f\"{label}: {index}\")\n",
        "\n",
        "      return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function to Encode Categorical Columns using One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BoTrgF8g40_K"
      },
      "outputs": [],
      "source": [
        "    def oneHotEncoding(self, df, categorical_cols):\n",
        "\n",
        "      from pyspark.ml import Pipeline # Module to create a pipeline of stages\n",
        "\n",
        "      from pyspark.ml.feature import StringIndexer, OneHotEncoder # Module to perform various feature engineering operations\n",
        "\n",
        "      from pyspark.sql.types import BooleanType # Module to specify the data type of a column\n",
        "\n",
        "      for column in categorical_cols:\n",
        "        # Check if the column has only two unique values\n",
        "        if df.schema[column].dataType == BooleanType():\n",
        "          df = df.withColumn(column, df[column].cast('int'))\n",
        "        else:\n",
        "          # String Indexing\n",
        "          indexer = StringIndexer(inputCol=column, outputCol=f\"{column}Index\")\n",
        "\n",
        "          # Hot Encoding\n",
        "          encoder = OneHotEncoder(inputCol=f\"{column}Index\", outputCol=f\"{column}Vec\")\n",
        "\n",
        "          # Create a pipeline\n",
        "          pipeline = Pipeline(stages=[indexer, encoder])\n",
        "\n",
        "          # Fit the pipeline and transform the data\n",
        "          pipeline_model = pipeline.fit(df)\n",
        "          df = pipeline_model.transform(df)\n",
        "\n",
        "          df = df.withColumn(column, df[f\"{column}Vec\"]).drop(df[f\"{column}Index\"], df[f\"{column}Vec\"])\n",
        "\n",
        "          # indexer_model = pipeline_model.stages[0]  # Get the StringIndexerModel from the pipeline\n",
        "          # labels = indexer_model.labels  # Get the list of labels\n",
        "          # print(\"Category to Index Mapping:\")\n",
        "          # for index, label in enumerate(labels):\n",
        "          #   print(f\"{label}: {index}\")\n",
        "\n",
        "      return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4d8z8C-apxp"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  from pyspark.sql import SparkSession  # Module to Create Spark Session\n",
        "  import matplotlib.pyplot as plt  # Module to plot graphs\n",
        "\n",
        "\n",
        "  spark = SparkSession.builder.master(\"local\").appName(\"DataPreprocessing\").getOrCreate()\n",
        "  \n",
        "  dp = DataPreprocessing()\n",
        "\n",
        "\n",
        "  df = dp.readCsv(\"//adult.csv\", spark)\n",
        "\n",
        "\n",
        "\n",
        "  target = df.columns[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZCq27jV1M5cc"
      },
      "outputs": [],
      "source": [
        "  if 'id' in df.columns:\n",
        "    df = df.drop('id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR-k4Bbjh30p",
        "outputId": "f3d78f5a-ea6c-486a-eac3-4cd4091aceee"
      },
      "outputs": [],
      "source": [
        "  df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxoGH-P3yW72",
        "outputId": "b66f2215-da93-451b-bd50-cc8bd70841a6"
      },
      "outputs": [],
      "source": [
        "  distinct_values = df.groupBy(target).count()\n",
        "  distinct_values.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7m-Chc-ngWr",
        "outputId": "fa52f3f8-0004-4666-fe89-61fbba2d0297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
            "['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'income']\n"
          ]
        }
      ],
      "source": [
        "  numeric_cols = []\n",
        "  categorical_cols = []\n",
        "\n",
        "  for column in df.dtypes:  # Check the data type of each column\n",
        "    if column[1] == 'string' or column[1] == 'boolean':  # If the data type is string, it is categorical\n",
        "      categorical_cols.append(column[0])\n",
        "    else:  # If the data type is numeric, it is numeric\n",
        "      numeric_cols.append(column[0])\n",
        "  print(numeric_cols)\n",
        "  print(categorical_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA3UFp7uvbgC",
        "outputId": "6ce14e1f-092d-4742-8693-47ab67187f7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32561\n"
          ]
        }
      ],
      "source": [
        "  print(df.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BklDUWqmvNq2"
      },
      "outputs": [],
      "source": [
        "  df = dp.fillMissingValues(df, numeric_cols, categorical_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k-nlua8WvevD",
        "outputId": "e1365262-99a3-451c-c2d5-0bb09e91317c"
      },
      "outputs": [],
      "source": [
        "  for column in numeric_cols:\n",
        "    mean_value_before = df.selectExpr(f'mean({column}) as mean').collect()[0]['mean']\n",
        "    stddev_value_before = df.selectExpr(f'stddev({column}) as stddev').collect()[0]['stddev']\n",
        "    # Plotting before outlier removal\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(df.select(column).rdd.flatMap(lambda x: x).collect(), bins=20, color='skyblue', edgecolor='black')\n",
        "    plt.axvline(x=mean_value_before, color='red', linestyle='--', label='Mean')\n",
        "    plt.axvline(x=mean_value_before + 3 * stddev_value_before, color='orange', linestyle='--', label='3 Std Dev')\n",
        "    plt.axvline(x=mean_value_before - 3 * stddev_value_before, color='orange', linestyle='--')\n",
        "    plt.title('Distribution Before Outlier Removal')\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AMhrLpxketY6"
      },
      "outputs": [],
      "source": [
        "  columns = numeric_cols[:]\n",
        "  if target in columns:\n",
        "    columns.remove(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBThIhaFUCUR",
        "outputId": "65baa98d-c409-4a73-d8ef-a2f1dc3712a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29829\n"
          ]
        }
      ],
      "source": [
        "  df = dp.RemoveOutliers_ZScore(df, columns)\n",
        "  print(df.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p9wnMoa6UFzC",
        "outputId": "4086156b-ec3a-40e5-82d3-f1b96ab2f21a"
      },
      "outputs": [],
      "source": [
        "  for column in numeric_cols:\n",
        "    mean_value_after = df.selectExpr(f'mean({column}) as mean').collect()[0]['mean']\n",
        "    stddev_value_after = df.selectExpr(f'stddev({column}) as stddev').collect()[0]['stddev']\n",
        "    # Plotting before outlier removal\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(df.select(column).rdd.flatMap(lambda x: x).collect(), bins=20, color='skyblue', edgecolor='black')\n",
        "    plt.axvline(x=mean_value_after, color='red', linestyle='--', label='Mean')\n",
        "    plt.axvline(x=mean_value_after + 3 * stddev_value_after, color='orange', linestyle='--', label='3 Std Dev')\n",
        "    plt.axvline(x=mean_value_after - 3 * stddev_value_after, color='orange', linestyle='--')\n",
        "    plt.title('Distribution after Outlier Removal')\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ9W6zNhYkeN",
        "outputId": "8e84d2be-7ff2-46dd-ecbe-6d283e44d23d"
      },
      "outputs": [],
      "source": [
        "  df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa6nsfcHTBu-",
        "outputId": "098d5189-170f-4fe2-eb95-9fb37cb4a429"
      },
      "outputs": [],
      "source": [
        "  df = dp.oneHotEncoding(df, categorical_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnWEBJHFUTr3",
        "outputId": "a0f290e5-50fa-4e1d-a2fc-b8564885c0e9"
      },
      "outputs": [],
      "source": [
        "  df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2WE8PWQUcQxQ"
      },
      "outputs": [],
      "source": [
        "  df = dp.LabelEncoding(df, categorical_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0E1NY3kcd-f",
        "outputId": "4b3888e7-8351-4b39-f657-7fa2c27a44b1"
      },
      "outputs": [],
      "source": [
        "  df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "2GF7Ct3LT6IK"
      },
      "outputs": [],
      "source": [
        "  columns.extend(categorical_cols)\n",
        "  if target in columns:\n",
        "    columns.remove(target)\n",
        "  df = dp.NormalizeNumericColumns(df, columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPaEc3ccpADd",
        "outputId": "5179576a-eb90-4afc-cadf-861e9f1312a8"
      },
      "outputs": [],
      "source": [
        "  df.select(\"Scaledfeatures\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EFI1yQkKVTA-",
        "outputId": "7b6be903-e442-4e31-fcc5-b87c4c24d8e7"
      },
      "outputs": [],
      "source": [
        "  for column in numeric_cols:\n",
        "    mean_value_after = df.selectExpr(f'mean({column}) as mean').collect()[0]['mean']\n",
        "    stddev_value_after = df.selectExpr(f'stddev({column}) as stddev').collect()[0]['stddev']\n",
        "    # Plotting before outlier removal\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(df.select(column).rdd.flatMap(lambda x: x).collect(), bins=20, color='skyblue', edgecolor='black')\n",
        "    plt.axvline(x=mean_value_after, color='red', linestyle='--', label='Mean')\n",
        "    plt.axvline(x=mean_value_after + 3 * stddev_value_after, color='orange', linestyle='--', label='3 Std Dev')\n",
        "    plt.axvline(x=mean_value_after - 3 * stddev_value_after, color='orange', linestyle='--')\n",
        "    plt.title('Distribution after Outlier Removal')\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw9WDmBtwjsd"
      },
      "outputs": [],
      "source": [
        "  spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
